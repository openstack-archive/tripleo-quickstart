# We're going to try putting files in `local_working_dir`, so make
# sure it exists first.
- name: Ensure local working dir exists
  delegate_to: localhost
  file:
    path: "{{ local_working_dir }}"
    state: directory

# Generate MAC addresses for the undercloud node.
- name: get MACs for the undercloud
  generate_macs:
    nodes:
      - "{{ undercloud_node }}"
    networks: "{{ networks }}"
  register: undercloud_mac_map

# Check if the undercloud volume exists.  If not, we call out to
# [fetch_image.yml](fetch_image.yml.html) to download the image.
- name: Check if undercloud volume exists
  command: >
    virsh vol-info --pool '{{ libvirt_volume_pool }}'
    '{{ undercloud_node.name }}.qcow2'
  ignore_errors: true
  register: undercloud_vol_check
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

- when: image_url is defined
  block:

  - name: Print warning about image_url deprecation
    debug:
      msg: image_url is deprecated please use undercloud_image_url

  - name: Set undercloud_image_url to image_url for backwards compatibility
    set_fact:
      undercloud_image_url: "{{ image_url }}"

# Conditionally include a playbook for all the images specified
# in options that downloads, cache and extract if tar archived
# only if the images aren't already in volume pool
- include: fetch_image.yml image_item={{ item }}
  when: undercloud_vol_check|failed
  with_items: "{{ images }}"

- include: inject_gating_repo.yml
  when: compressed_gating_repo is defined

# This copies the `instackenv.json` configuration file that we
# generated in the overcloud setup role to the undercloud host.
- name: Copy instackenv.json to appliance
  command: >
    virt-customize -a {{ working_dir }}/undercloud.qcow2
    --upload {{ working_dir }}/instackenv.json:/home/stack/instackenv.json
  environment:
    LIBGUESTFS_BACKEND: direct
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"
  when: undercloud_vol_check|failed

# Copy the undercloud public key to the virthost, because we're going
# to inject it into the undercloud image in the next task.
- name: Copy undercloud ssh public key to working dir
  copy:
    src: "{{ undercloud_key }}.pub"
    dest: "{{ working_dir }}/id_rsa_undercloud.pub"
  when: undercloud_vol_check|failed

# Copy the public key to `$HOME/.ssh/authorized_keys` for the `root`
# and `stack` user on the undercloud.
- name: Inject undercloud ssh public key to appliance
  command: >
    virt-customize -a {{ working_dir }}/undercloud.qcow2
    --mkdir {{item.homedir}}/.ssh/
    --upload '{{ working_dir }}/id_rsa_undercloud.pub:{{item.homedir}}/.ssh/authorized_keys'
    --run-command 'chown -R {{item.owner}}:{{item.group}} {{item.homedir}}/.ssh'
    --run-command 'chmod 0700 {{item.homedir}}/.ssh'
    --run-command 'chmod 0600 {{item.homedir}}/.ssh/authorized_keys'
  environment:
    LIBGUESTFS_BACKEND: direct
  with_items:
    - homedir: /root
      owner: root
      group: root
    - homedir: /home/stack
      owner: stack
      group: stack
  when: undercloud_vol_check|failed

# Perform an SELinux relabel on the undercloud image to avoid problems
# caused by bad labelling, since by default the undercloud runs in
# enforcing mode.
- name: Perform selinux relabel on undercloud image
  command: >
    virt-customize -a {{ working_dir }}/undercloud.qcow2
    --selinux-relabel
  environment:
    LIBGUESTFS_BACKEND: direct
  when: undercloud_vol_check|failed

# This copies the overcloud and ipa images if we are not going to build
# them
- name: Inject additional images
  shell: >
    virt-customize -a {{ working_dir }}/undercloud.qcow2
    --upload {{ working_dir }}/{{ item }}:/home/stack/{{ item }};
  environment:
    LIBGUESTFS_BACKEND: direct
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"
  when: download_overcloud_images|bool
  with_items: "{{ inject_images }}"

# Resize the undercloud image to our desired size.
- name: Resize undercloud image (create target image)
  command: >
    qemu-img create -f qcow2 -o preallocation=off
    '{{ working_dir }}/undercloud-resized.qcow2'
    '{{ flavors[undercloud_node.name].disk }}G'
  when: undercloud_vol_check|failed

- name: Resize undercloud image (call virt-resize)
  command: >
    virt-resize --expand /dev/sda1
    '{{ working_dir }}/undercloud.qcow2'
    '{{ working_dir }}/undercloud-resized.qcow2'
  environment:
    LIBGUESTFS_BACKEND: direct
  when: undercloud_vol_check|failed

# Create a libvirt volume and upload the undercloud image to
# libvirt.
- name: Create undercloud volume
  command: >
    virsh vol-create-as {{ libvirt_volume_pool}}
    {{ undercloud_node.name }}.qcow2 \
    {{ flavors[undercloud_node.name].disk }}G --format qcow2
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"
  when: undercloud_vol_check|failed

- name: Upload undercloud volume to storage pool
  command: >
    virsh -k 0 vol-upload --pool '{{ libvirt_volume_pool }}'
    '{{ undercloud_node.name }}.qcow2'
    '{{ working_dir }}/undercloud-resized.qcow2'
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"
  async: 600
  poll: 10
  when: undercloud_vol_check|failed

# Define (but do no start) the undercloud virtual machine.
- name: Define undercloud vm
  virt:
    name: "{{ undercloud_node.name }}"
    command: define
    xml: "{{ lookup('template', 'undercloudvm.xml.j2') }}"
    uri: "{{ libvirt_uri }}"

# Start the undercloud virtual machine.
- name: Start undercloud vm
  virt:
    name: "{{ undercloud_node.name }}"
    command: start
    state: running
    uri: "{{ libvirt_uri }}"

# Get the ip address of the undercloud.  This will retry several times
# (`undercloud_ip_retries`) until the undercloud is ready.  The script
# works by getting the MAC address of the first undercloud interface,
# and then looking that up in the kernel ARP table.
- name: Get undercloud vm ip address
  script: "scripts/get-undercloud-ip.sh {{ undercloud_node.name }}"
  register: undercloud_vm_ip_result
  until: undercloud_vm_ip_result|success
  retries: "{{ undercloud_ip_retries }}"
  delay: 10
  environment:
    LIBVIRT_DEFAULT_URI: "{{ libvirt_uri }}"

- name: Set_fact for undercloud ip
  set_fact:
    undercloud_ip: "{{ undercloud_vm_ip_result.stdout_lines[0] }}"

- name: Wait until ssh is available on undercloud node
  wait_for:
    host: "{{ undercloud_ip }}"
    state: started
    port: 22
    timeout: 300

# Add the undercloud to the in-memory inventory.
- name: Add undercloud vm to inventory
  add_host:
    name: undercloud
    groups: undercloud
    ansible_host: undercloud
    ansible_fqdn: undercloud
    ansible_user: stack
    ansible_private_key_file: "{{ undercloud_key }}"
    ansible_ssh_extra_args: '-F "{{local_working_dir}}/ssh.config.ansible"'
    undercloud_ip: "{{ undercloud_ip }}"

- name: Generate ssh configuration
  delegate_to: localhost
  template:
    src: ssh.config.j2
    dest: "{{ local_working_dir }}/ssh.config.ansible"

# Ironic defaults to using `qemu:///system`.  When running libvirtd
# unprivileged we need to use `qemu:///session`.  This allows us to pass
# the value of libvirt_uri into /etc/ironic/ironic.conf.
- name: Configure Ironic pxe_ssh driver
  delegate_to: undercloud
  ini_file:
    dest: /etc/ironic/ironic.conf
    section: ssh
    option: libvirt_uri
    value: '{{ libvirt_uri }}'
  become: true
